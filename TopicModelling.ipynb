{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tematsko modeliranje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tematsko modeliranje (engl. *topic modelling*) je vrsta statističkog modeliranja čiji je cilj pronalaženje tema u okviru korpusa dokumenata. Drugim rečima, u okviru korpusa dokumenata $\\mathcal{D}$, tražimo skup tema $ T $ da bismo dokumente mogli adekvatno sortirati. Za razliku od problema klasifikacije teksta, gde je skup tema već poznat, tematsko modeliranje je problem nenadgledanog učenja. Dva osnovna pristupa rešavanju problema tematskog modeliranja jesu LSA i LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predprocesiranje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bi se tehnike LSA i LDA mogle koristiti, podaci se moraju pripremiti na odgovarajući način. Prvo što je potrebno uraditi jeste predstavljanje dokumenata preko vektora gde svaka vrednost predstavlja koliko se puta reč nalazi u skupu reči:\n",
    "\n",
    "$ \\textrm{\"the throne of the king\"} \\longrightarrow  \n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    \\vdots \\\\\n",
    "    1\n",
    "\\end{bmatrix} \\in\n",
    "\\begin{bmatrix}\n",
    "    the \\\\\n",
    "    live \\\\\n",
    "    reign \\\\\n",
    "    throne \\\\\n",
    "    \\vdots \\\\\n",
    "    king\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Ovde se koristi pristup sa multiskupom reči; svaka reč se svodi na broj pojavljivanja, dok se pozicija reči u tekstu zanemaruje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pošto postoji $n$ dokumenata, dobija se matrica reda $N \\times k$, gde je $k$ broj reči u biblioteci. Svaki dokument odgovaraće jednom redu rezultujuće matrice:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "    \\textrm{\"the throne of the king\"} \\\\\n",
    "    \\textrm{\"a new reign\"} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\textrm{\"long live the king\"}\n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\n",
    "\\begin{bmatrix}\n",
    "    2 & 0 & 0 & 1 & \\dots & 1 \\\\\n",
    "    0 & 0 & 1 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    1 & 1 & 0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Primer iznad je dat radi vizuelizacije podataka, ali se iz teksta generalno odbacuju tzv. stop reči (česte reči bez izrazitog značenja poput \"*a*\" ili \"*the*\") . Izuzev njih, za reči koje ulaze u sastav skupa se biraju reči koje se najčešće pojavljuju."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za potrebe demonstriranja će biti korišćen skup podataka \"*20 Newsgroup*\" u okviru biblioteke `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "documents = dataset.data\n",
    "\n",
    "print(len(documents)) \n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postoji 11314 dokumenata koji pripadaju jednoj od prethodno navedenih 20 grupa vesti. Dalje je potrebno izbrisati sve karaktere koji nisu slova ili razmak; a nakon toga, treba prebaciti sve karaktere u mala slova. Bilo bi poželjno i izbaciti kratke reči (do 3 slova):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document\n",
      "0  well sure about story seem biased what disagre...\n",
      "1  yeah expect people read actually accept hard a...\n",
      "2  although realize that principle your strongest...\n",
      "3  notwithstanding legitimate fuss about this pro...\n",
      "4  well will have change scoring playoff pool unf...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Najjednostavniji način da se postigne željenu rezultat je\n",
    "# da koristimo DataFrame strukturu\n",
    "news_df = pd.DataFrame({\"document\":documents})\n",
    "\n",
    "# Koristimo regex za odstranjivanje svih karaktera koji nisu slova\n",
    "news_df[\"document\"] = news_df[\"document\"].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Izbacivanje kratkih reci\n",
    "news_df[\"document\"] = news_df[\"document\"].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "\n",
    "# Pretvaranje svih slova u mala slova\n",
    "news_df[\"document\"] = news_df[\"document\"].apply(lambda x: x.lower())    \n",
    "\n",
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bi se pomenuta matrica napravila, može se koristiti objekat `CountVectorizer` u okviru `sklearn` biblioteke. U nastavku će biti korišćen blago modifikovani pristup sa korišćenjem **tfidf** težina, pa će se koristiti `TfidfVectorizer` koji računa težinu svake reči za svaki dokument po sledećoj formuli:\n",
    "\n",
    "$ w_{i,j} = tf_{i,j} \\log{\\dfrac{N}{df_j}} $\n",
    "\n",
    "gde su $ tf_{i,j} $ broj ponavljanja reči $j$ u dokumentu $i$, a $ df_{j} $ broj ponavljanja reči $j$ u svim dokumentima u korpusu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(11314, 1000)\n",
      "  (0, 664)\t0.12482699217899616\n",
      "  (0, 444)\t0.15920786810198112\n",
      "  (0, 497)\t0.11606278893174055\n",
      "  (0, 73)\t0.13042573485326867\n",
      "  (0, 516)\t0.13226489554872167\n",
      "  (0, 366)\t0.12704052323102857\n",
      "  (0, 716)\t0.15952691353774057\n",
      "  (0, 814)\t0.1846235059899824\n",
      "  (0, 732)\t0.16381332141612362\n",
      "  (0, 154)\t0.1582696239800608\n",
      "  (0, 730)\t0.16151197900355305\n",
      "  (0, 712)\t0.128507139034102\n",
      "  (0, 893)\t0.0880418781548271\n",
      "  (0, 477)\t0.16834732334900732\n",
      "  (0, 229)\t0.17099938915691926\n",
      "  (0, 710)\t0.16609758611560899\n",
      "  (0, 388)\t0.12393127093119301\n",
      "  (0, 985)\t0.12177290184053344\n",
      "  (0, 438)\t0.34482777905476636\n",
      "  (0, 532)\t0.6620433868499663\n",
      "  (0, 841)\t0.15289850944254976\n",
      "  (0, 849)\t0.1573584484023006\n",
      "  (0, 867)\t0.11171887866892631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "M = 1000 # Maksimalni broj kolona\n",
    "tv = TfidfVectorizer(stop_words=\"english\", max_features=M)\n",
    "\n",
    "document_term_matrix = tv.fit_transform(news_df[\"document\"])\n",
    "print(type(document_term_matrix))\n",
    "print(document_term_matrix.shape)\n",
    "print(document_term_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ova matrica se u literaturi često naziva dokument-term matrica (engl. *Document-term matrix*). Primetimo da se ona predstavlja sa CSR modelom koji reprezentuje retke matrice, što ima smisla, jer ova matrica ima malo nenula elemenata u odnosu na njenu veličinu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA (engl. *Latent Semantic Analysis*) se zasniva na smanjivanju dimenzionalnosti dobijene dokument-term matrice $D$ uz pomoć singularne dekompozicije:\n",
    "\n",
    "$ D = U \\Sigma V^T $\n",
    "\n",
    "Da bismo dobili manju dimenziju matrice $D$ možemo napraviti aproksimaciju te matrice tako što ćemo \"odseći\" deo manje bitnih informacija (po parametru $t$):\n",
    "\n",
    "$ D \\approx U_t {\\Sigma}_t V^T_t $\n",
    "\n",
    "gde su $U_t$, ${\\Sigma}_t$ i $V^T_t$ matrice veličina $ N \\times t $, $ t \\times t $ i $ t \\times N $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U *python*-u, ovo se može izvesti pomoću `TruncatedSVD` objekta iz biblioteke `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: just like know people think does good\n",
      "Topic 2: thanks windows card drive mail file advance\n",
      "Topic 3: game team year games drive season good\n",
      "Topic 4: drive scsi disk hard problem drives just\n",
      "Topic 5: drive know does thanks just scsi drives\n",
      "Topic 6: just like windows know window does file\n",
      "Topic 7: just like mail bike thanks chip email\n",
      "Topic 8: does know chip like clipper card encryption\n",
      "Topic 9: like card sale good offer video jesus\n",
      "Topic 10: like drive file files space program sounds\n",
      "Topic 11: people like card government thanks windows right\n",
      "Topic 12: think chip clipper encryption good need mail\n",
      "Topic 13: good problem thanks bike know right window\n",
      "Topic 14: good know windows people file files sale\n",
      "Topic 15: space know nasa think year shuttle launch\n",
      "Topic 16: does israel think israeli right jews sale\n",
      "Topic 17: good space card thanks does year people\n",
      "Topic 18: problem people window using year server know\n",
      "Topic 19: file problem does files people thanks make\n",
      "Topic 20: right sure mail want make year believe\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def LSA(data, t):\n",
    "    svd_model = TruncatedSVD(n_components=t, algorithm=\"randomized\", random_state=10)\n",
    "    data = svd_model.fit_transform(data)\n",
    "    \n",
    "    return (svd_model.components_, data)\n",
    "\n",
    "def print_topics(components, vectorizer, n_words):\n",
    "    #dohvatamo M originalnih tema\n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "    for i, comp in enumerate(components):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        sorted_terms = map(lambda x: x[0], sorted_terms)\n",
    "        \n",
    "        topics = \" \".join(sorted_terms)\n",
    "        print(f\"Topic {i+1}: \" + topics)\n",
    "\n",
    "# Očekujemo 20 tema\n",
    "components, lsa_data = LSA(document_term_matrix, 20)\n",
    "print_topics(components, tv, 7)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLSA (engl. *Probabilistic Latent Semantic Analysis*) ne koristi SVD dekompoziciju, već se problemu prisupa probabilistički. Ovde želimo da napravimo probabilistički model $P$ tako da za svaki dokument $d$ i reč $w$ važi da $P(d,w)$ odgovara adekvatnom elementu matrice $D$. Formalno, možemo to matematički izraziti na sledeći način:\n",
    "\n",
    "$ P(d,w) = P(d) \\sum_{z} P(z|d) P(w|z) $\n",
    "\n",
    "U prethodnoj formuli, z predstavlja temu, $P(d)$ je verovatnoća posmatranja dokumenta $d$, $P(z|d)$ je verovatnoća da $z$ bude tema tog dokumenta, a $P(w|z)$ je verovatnoća da se reč $w$ nalazi u dokumentu teme $z$.\n",
    "\n",
    "PLSA je fleksibilniji model od LSA, ali ima svoje nedostatke: na primer, broj parametara raste linearno sa brojem dokumenata u korpusu, te je ovaj model sklon preprilagođavanju (engl. *overfitting*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (engl. Latent Dirichlet Allocation) je pandan PLSA algoritmu čija je osnova Bajesova verovatnoća. U suštini, koristi se obrnuti pristup: pokušavamo da generišemo dokumente tako što predpostavljamo raspodelu verovatnoće da određena dokument sadrži reči koje pripadaju nekoj temi, kao i raspodelu verovatnoća da neke reči pripadaju određenim temama.\n",
    "\n",
    "Uvedimo naredne oznake (i podsetimo se starih):\n",
    "\n",
    "- $t$ — broj tema po dokumentu (fiksirana vrednost)\n",
    "- $k$ — broj reči u biblioteci\n",
    "- $N$ — broj dokumenata\n",
    "- $M$ — broj reči u svakom dokumentu\n",
    "- $w$ — reč u dokumentu\n",
    "- $d$ — dokument\n",
    "- $\\mathcal{D}$ — korpus dokumenata\n",
    "- $z$ — tema koja je predstavljena kao distribucija reči\n",
    "- $Z$ — sve teme dokumenta\n",
    "\n",
    "Uzmimo parametar $\\alpha$, gde je $ 0 < \\alpha < 1 $. Uz pomoć parametra $\\alpha$ možemo da generišemo matricu $\\theta$ čiji element $\\theta_{i,j}$ označava verovatnoću da $i$-ti dokument ima reči koje pripadaju $j$-toj temi.\n",
    "\n",
    "Slično, uz pomoć parametra $\\beta$ generišemo matricu $\\phi$, čiji element $\\phi_{i,j}$ je jednak verovatnoći da $i$-toj temi pripada $j$-ta reč."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Način na koji generišemo raspodele $\\theta$ i $\\phi$ je uz pomoć **Dirihleove raspodele**. Ova raspodela uglavnom uzima više parametara, ali će u našem slučaju parametri biti isti, te ćemo pisati samo jedan parametar.\n",
    "\n",
    "Dirihleova raspodela je pogodna za ovaj problem zbog toga što možemo kontrolisati kakvu će raspodelu generisati: što je parametar bliži 1, to će i raspodela koju će generisati biti bliža uniformnoj raspodeli; a što je parametar bliži 0, to znači da će većina verovatnoća u raspodeli biti bliska nuli (naravno, to implicira da nekolicina verovatnoća \"odskače\" od ostalih). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ono što je potrebno naći u ovom momentu jesu parametri $\\alpha$ i $\\beta$. Dakle, tražimo distribuciju tema $\\theta$ (za svaki dokument), sve teme za svaki dokument $Z$ i distribuciju reči $\\phi$ (za svaku temu) ako imamo korpus $\\mathcal{D}$ i koristimo parametre $\\alpha$ i $\\beta$. Matematički napisano, to izgleda ovako:\n",
    "\n",
    "$ P(\\theta, Z, \\phi | \\mathcal{D}; \\alpha, \\beta)$\n",
    "\n",
    "Međutim, problem je u tome što je ovo nemoguće izračunati. Ali, možemo aproksimirati ovaj problem problemom optimizacije tzv. *KL divergencije*:\n",
    "\n",
    "$ \\gamma^*, \\xi^*, \\lambda^* = argmin_{(\\gamma, \\xi, \\lambda)} D_{KL}(q(\\theta,Z,\\phi|\\gamma, \\xi, \\lambda) || p(\\theta, Z, \\phi | \\mathcal{D}; \\alpha, \\beta))$\n",
    "\n",
    "Sa $\\gamma$, $\\xi$ i $\\lambda$ aproksimiramo $\\theta$, $Z$ i $\\phi$ redom; $D_{KL}(q||p)$ označava KL divergenciju između $q$ i $p$. \n",
    "\n",
    "Ukratko, potrebno je da nađemo $\\gamma$, $\\xi$ i $\\lambda$ tako da minimiziramo KL divergenciju između prave posteriorne verovatnoće $p$ i njene aproksimacije $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korišćenje LDA u programskom jeziku Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA nije neophodno implementirati \"ručno\"; za programski jezik *Python* postoji objekat `LatentDirichletAllocation` u okviru biblioteke `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: info clock speed site appreciated thanks directory\n",
      "Topic 2: jesus bible christ list christians people know\n",
      "Topic 3: bike just like cars good engine know\n",
      "Topic 4: university list available mail internet information send\n",
      "Topic 5: space nasa address moon launch orbit shuttle\n",
      "Topic 6: game team year games play season players\n",
      "Topic 7: sale condition excellent offer asking miles wire\n",
      "Topic 8: chip encryption clipper chips phone data quality\n",
      "Topic 9: card video drivers monitor cards modem driver\n",
      "Topic 10: drive shipping disk drives hard sale floppy\n",
      "Topic 11: science food theory blood patients like know\n",
      "Topic 12: deleted stuff thought lots colorado somebody john\n",
      "Topic 13: mike water drugs drug agree hours company\n",
      "Topic 14: people think government just like israel right\n",
      "Topic 15: thanks advance interested email mail looking dave\n",
      "Topic 16: scsi apple thanks software keyboard know number\n",
      "Topic 17: file files windows program output thanks need\n",
      "Topic 18: window windows problem screen using color display\n",
      "Topic 19: just believe think people true wrong doesn\n",
      "Topic 20: article like road models remember getting look\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components=20)\n",
    "lda.fit(document_term_matrix)\n",
    "print_topics(lda.components_, tv, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takođe postoji i biblioteka `gensim` koja implementira svoju verziju LDA algoritma; ali kako smo već izračunali dokument-term matricu, korišćenje verzije iz `sklearn` je dosta olakšano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaključak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA algoritam jeste intutivniji, ali je činjenica da je LDA metoda dosta popularnija i neretko efektivnija metoda tematskog modeliranja. Zgodna stvar kod ovih metoda jeste postojanje različitih gotovih implementacija, pa se vrlo jednostavno mogu koristiti kao \"crne kutije\"; te se problemi ovog tipa, koji su sve češći, mogu brzo i efikasno rešavati."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitdc7810f74838480dabcd7f02ae4f8b21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
